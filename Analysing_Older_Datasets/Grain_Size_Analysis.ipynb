{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6daa5f-1edf-4b63-8e6e-828d1fd2ebeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mantid.simpleapi import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import lmfit as lm\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime as dt\n",
    "import pathlib\n",
    "from mantid.geometry import ReflectionGenerator, ReflectionConditionFilter\n",
    "from numpy.polynomial import Chebyshev as cheb\n",
    "\n",
    "#This is the location of the SummaryTxt file\n",
    "SummaryTxt = r'\\\\isis\\inst$\\NDXENGINX\\Instrument\\logs\\journal\\SUMMARY.TXT'\n",
    "\n",
    "#This is the location of the files with the Collimator Groupings\n",
    "GroupingFiles = r'C:\\MantidInstall\\scripts\\Engineering\\calib'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a24f46-3541-412e-b5e3-a22fa8d749ca",
   "metadata": {},
   "source": [
    "HDF files are a very useful and compact way to store 3D Data in a form that users can easily interact with. This cell creates the initial HDF file, as they can be very specific about naming, and after the data is filled in with other functions, a table is appended of all the parameters against all the runs. There is also a function for saving searches, peaks across banks, and orientation relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d6fcea-0b81-4f27-85bd-c5ec124f9e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createHDF(name):\n",
    "    fn = f'{name}.hdf5'\n",
    "    check = pathlib.Path(fn)\n",
    "    i = 1\n",
    "    while check.exists():\n",
    "        new_name = f'{fn[0:-5]}({i}).hdf5'\n",
    "        check = pathlib.Path(new_name)\n",
    "        i += 1\n",
    "    return check\n",
    "\n",
    "def saveSearch(name, search, df):\n",
    "    currdir = os.getcwd()\n",
    "    savedir = os.path.join(currdir, \"Saves\")\n",
    "    newdir = os.path.join(savedir, str(name))\n",
    "    if not os.path.exists(newdir):\n",
    "        os.makedirs(newdir)\n",
    "    filedir = os.path.join(newdir,str(name))\n",
    "    file = createHDF(filedir)\n",
    "    dic = df.to_dict('list')\n",
    "    with h5py.File(str(file),\"w\") as f:\n",
    "        grp = f.create_group(search)\n",
    "        for ColumnName in ['Inst','5DigitRunNumber','Users','Title','Date','Time','uAh','RB No.']:\n",
    "            datset = f.create_dataset(f'{search}/{ColumnName}',data=dic[f'{ColumnName}'])\n",
    "    f.close()\n",
    "    #appendTableTwo(file)\n",
    "\n",
    "def storeGrpPeaks(name, df, peaklen,hkls):\n",
    "#def storeGrpPeaks(name,df,peaklen):\n",
    "    currdir = os.getcwd()\n",
    "    savedir = os.path.join(currdir, \"Saves\")\n",
    "    newdir = os.path.join(savedir, str(name))\n",
    "    if not os.path.exists(newdir):\n",
    "        os.makedirs(newdir)\n",
    "    filedir = os.path.join(newdir,str(name))\n",
    "    file = createHDF(filedir)\n",
    "    dic = df.to_dict('list')\n",
    "    #print(dic)\n",
    "    with h5py.File(str(file),\"w\") as f:\n",
    "        #for i in range(peaklen):\n",
    "        for i in range(len(hkls)):\n",
    "            peakPara = []\n",
    "            grp = f.create_group(f'{hkls[i]}-Peak')\n",
    "            for ColumnName in ['Height', 'Height_Err', 'PeakCentre', 'PeakCentre_Err', 'Sigma', 'Sigma_Err', 'Integrated Intensity', 'Integrated Intensity_Err', 'Chi_squared']:\n",
    "                #datset = f.create_dataset(f'Peak No.{k}/{ColumnName}', data=dic[f'Peak No.{k}-{ColumnName}'])\n",
    "                try:\n",
    "                    datset = f.create_dataset(f'{hkls[i]}-Peak/{ColumnName}', data=dic[f'{hkls[i]}-{ColumnName}'])\n",
    "                except KeyError:\n",
    "                    print(f\"Can't save {hkls[i]}-{ColumnName} into HDF. This may be due to an error with the Peak Detection.\")\n",
    "                    pass\n",
    "    f.close()\n",
    "    try:\n",
    "        appendTable(file)\n",
    "    except IndexError:\n",
    "        pass\n",
    "\n",
    "def saveOrientation(name,corr,table,peaklen,orderDic,peaks):\n",
    "    currdir = os.getcwd()\n",
    "    savedir = os.path.join(currdir, \"Saves\")\n",
    "    newdir = os.path.join(savedir, str(name))\n",
    "    if not os.path.exists(newdir):\n",
    "        os.makedirs(newdir)\n",
    "    filedir = os.path.join(newdir,f'{name}_Orientation_{corr}_Correlation')\n",
    "    file = createHDF(filedir)\n",
    "    with h5py.File(str(file),\"w\") as f:\n",
    "        grp = f.create_group(f'{name}-Correlations')\n",
    "        i = 0\n",
    "        for peak in peaks:\n",
    "            #grp = f.create_group(f'Peak No.{k}')\n",
    "            peakCorr = []\n",
    "            for j in range(peaklen):\n",
    "                peakCorr.append(table[i,j])\n",
    "            #print(peakCorr)\n",
    "            datset = f.create_dataset(f'{name}-Correlations/{peak}', data=peakCorr)\n",
    "            i = i + 1\n",
    "        tableset = f.create_dataset(f'{name}-Correlations/Full Table', shape=(peaklen,peaklen), maxshape=(None,None), data=table)\n",
    "    f.close()\n",
    "    orderedPeaksHDF(file,orderDic,peaks)\n",
    "\n",
    "def appendTable(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        keyList = list(f.keys())\n",
    "        #print(keyList)\n",
    "        rows = len(keyList)\n",
    "        cols = 0\n",
    "        for i in range(rows):\n",
    "            check = f[keyList[i]]\n",
    "            #print(check)\n",
    "            if keyList[i] == \"Tables\":\n",
    "                i = i + 1\n",
    "            else:\n",
    "                checkList = list(check)\n",
    "                params = len(checkList)\n",
    "                colTest = len(list(check[checkList[0]]))\n",
    "                #colTest = len(list(check[\"Inst\"]))\n",
    "                if colTest > cols:\n",
    "                    cols = colTest\n",
    "                else:\n",
    "                    cols = cols\n",
    "    f.close()\n",
    "    with h5py.File(filename, \"a\") as f:\n",
    "        for k in range(params):\n",
    "            path = \"Tables/\" + checkList[k]\n",
    "            tableData = f.create_dataset(path, (rows,cols), maxshape=(None,None), dtype='float')\n",
    "            for i in range(rows):\n",
    "                run = f[keyList[i]]\n",
    "                dat = list(run[checkList[k]])\n",
    "                diff = cols - len(dat)\n",
    "                for j in range(diff):\n",
    "                    dat.append(0)\n",
    "                tableData[i,:] = dat\n",
    "            \n",
    "    f.close()\n",
    "\n",
    "#This is to create a HDF of the ordered peaks\n",
    "def orderedPeaksHDF(filename, orderDic,peaks):\n",
    "    with h5py.File(filename,\"a\") as f:\n",
    "        for peak in peaks:\n",
    "            path = f'Ordered_Peaks/{peak}_ordered'\n",
    "            arr = orderDic[f'{peak}_ordered']\n",
    "            datset = f.create_dataset(path, data=arr)\n",
    "    f.close()\n",
    "        \n",
    "    \n",
    "#There's an issue with the types when making a table\n",
    "def appendTableTwo(filename):\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        keyList = list(f.keys())\n",
    "        #print(keyList)\n",
    "        rows = len(keyList)\n",
    "        cols = 0\n",
    "        for i in range(rows):\n",
    "            check = f[keyList[i]]\n",
    "            print(check)\n",
    "            if keyList[i] == \"Tables\":\n",
    "                i = i + 1\n",
    "            else:\n",
    "                checkList = list(check)\n",
    "                params = len(checkList)\n",
    "                colTest = len(list(check[checkList[0]]))\n",
    "                #colTest = len(list(check[\"Inst\"]))\n",
    "                if colTest > cols:\n",
    "                    cols = colTest\n",
    "                else:\n",
    "                    cols = cols\n",
    "    f.close()\n",
    "    with h5py.File(filename, \"a\") as f:\n",
    "        ds_dtype = []\n",
    "        for i in range(params):\n",
    "            checkType = type(checkList[i][0])\n",
    "            ds_dtype.append((f'{checkList[i]}',checkType))\n",
    "        #print(ds_dtype)\n",
    "        ds_arr = np.recarray((cols,),dtype=ds_dtype)\n",
    "        for i in range(params):\n",
    "            for j in range(rows):\n",
    "                run = f[keyList[j]]\n",
    "                dat = list(run[checkList[i]])\n",
    "                ds_arr[f'{checkList[i]}'] = np.asarray(dat)\n",
    "        dset = f.create_dataset('Table', dtype=ds_dtype, shape=(cols,), maxshape=(None) )\n",
    "        for i in range(params):\n",
    "            dset[f'{checkList[i]}',0:i] = np.asarray(list(run[checkList[i]]))\n",
    "            dset[f'{checkList[i]}',0:i] = np.asarray(list(run[checkList[i]]))\n",
    "            dset[f'{checkList[i]}',0:i] = np.asarray(list(run[checkList[i]]))\n",
    "            dset[f'{checkList[i]}',0:i] = np.asarray(list(run[checkList[i]]))\n",
    "            \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ca1f2-c63c-4910-8ccc-95c549ca1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itables import init_notebook_mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "\n",
    "#Borrow some of this code to improve filesearch\n",
    "def LoadSummaryTxt(Filename,col,search,sort):\n",
    "    col_names = ['Inst','5DigitRunNumber','Users','Title','Date','Time','uAh','RB No.']\n",
    "    col_width = [(0,3),(3,8),(8,28),(28,52),(52,63),(63,72),(72,80),(80,88)]\n",
    "    # Run 53079 on Engin-X has corrupted row\n",
    "    InvalidRows = [24493,41604,41605,41606,41607,41608]\n",
    "    datas = pd.read_fwf(Filename, names=col_names, colspecs=col_width, na_filter=False,  encoding='ANSI', skiprows=InvalidRows) # encoding='ISO-8859-1'\n",
    "    #print(datas)\n",
    "    df = pd.DataFrame(datas)\n",
    "    RunNumberDiff = df['5DigitRunNumber'].diff()\n",
    "    RunNumberDiff = RunNumberDiff.replace(-99999, 1).replace(-99998, 2) # Replace wraparound values\n",
    "    RunNumberDiff.iloc[0] = df['5DigitRunNumber'].iloc[0]\n",
    "    df['RunNumber'] = RunNumberDiff.cumsum().astype('int64')\n",
    "    df['uAh'] = pd.to_numeric(df['uAh'], errors='coerce')\n",
    "    df.set_index('RunNumber', inplace=True)\n",
    "    try:\n",
    "        searchResults = df[df[str(col)].str.contains(f'{search}')].sort_values(str(sort))\n",
    "    except AttributeError:\n",
    "        searchResults = df[str(col)].dtype\n",
    "    searchQuery = f'{col}-{search}'\n",
    "    hdfName = f'{search}_sort_by_{sort}'\n",
    "    #saveSearch(hdfName,searchQuery,searchResults)\n",
    "    #return df\n",
    "    return searchResults\n",
    "\n",
    "df = LoadSummaryTxt(SummaryTxt,\"Date\",\"2024\",\"RunNumber\")\n",
    "df\n",
    "#pd.to_numeric(df['RunNumber']).diff().sort_values()\n",
    "#df.plot.hist()\n",
    "#\"scan\" in df.Title\n",
    "#df\n",
    "#df[df[\"Title\"].str.contains(\"CeO2\")].sort_values('uAh')   #... ['uAh'].hist(bins=100)\n",
    "#df.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4f8798-9bb1-4350-a6c3-873cea471a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileSearch(run,exp,bank):\n",
    "    exps = ['Cryo','Furnace','Pressure','X Scan','Y Scan','Z Scan']\n",
    "    if len(str(run)) < 5:\n",
    "        zero_diff = 5 - len(str(run))\n",
    "        for i in range(zero_diff):\n",
    "            run = f'0{run}'\n",
    "            \n",
    "    yr = dt.now().strftime('%y')\n",
    "    cy = int(yr) + 1\n",
    "\n",
    "    if bank == '1':\n",
    "        sMin_One = 0\n",
    "        sMax_One = 1200\n",
    "        sMin_Two = 1\n",
    "        sMax_Two = 1199\n",
    "    elif bank == '2':\n",
    "        sMin_One = 1201\n",
    "        sMax_One = 2400\n",
    "        sMin_Two = 1200\n",
    "        sMax_Two = 2399\n",
    "    elif bank == 'Both':\n",
    "        sMin_One = 0\n",
    "        sMax_One = 2400\n",
    "        sMin_Two = 1\n",
    "        sMax_Two = 2399\n",
    "    else:\n",
    "        sMin_One = 0\n",
    "        sMax_One = 2400\n",
    "        sMin_Two = 1\n",
    "        sMax_Two = 2399\n",
    "    for i in reversed(range(2,int(cy))):\n",
    "        if i < 10:\n",
    "            year = f'0{i}'\n",
    "        else:\n",
    "            year = f'{i}'\n",
    "        if i < 17:\n",
    "            apps = [['cryo_1','cryo_temp1'],'furnace','position','x','y','z']\n",
    "            if exp == exps[0]:\n",
    "                if i < 10:\n",
    "                    app = apps[0][0]\n",
    "                else:\n",
    "                    app = apps[0][1]\n",
    "            elif exp == exps[1]:\n",
    "                app = apps[1]\n",
    "            elif exp == exps[2]:\n",
    "                app = apps[2]\n",
    "            elif exp == exps[3]:\n",
    "                app = apps[3]\n",
    "            elif exp == exps[4]:\n",
    "                app = apps[4]\n",
    "            elif exp == exps[5]:\n",
    "                app = apps[5]\n",
    "            elif exp == None:\n",
    "                pass\n",
    "            else:\n",
    "                raise NameError(f'This is not included in the list of experiments. Please enter one from this list: {exps}')\n",
    "        else:\n",
    "            apps = ['Temp_1','Temp_3','position','X_position','Y_position','Z_position']\n",
    "            if exp == exps[0]:\n",
    "                app = apps[0]\n",
    "            elif exp == exps[1]:\n",
    "                app = apps[1]\n",
    "            elif exp == exps[2]:\n",
    "                app = apps[2]\n",
    "            elif exp == exps[3]:\n",
    "                app = apps[3]\n",
    "            elif exp == exps[4]:\n",
    "                app = apps[4]\n",
    "            elif exp == exps[5]:\n",
    "                app = apps[5]\n",
    "            elif exp == None:\n",
    "                pass\n",
    "            else:\n",
    "                raise NameError(f'This is not included in the list of experiments. Please enter one from this list: {exps}')\n",
    "        for cycle in [5,4,3,2,1]:\n",
    "            if i < 8:\n",
    "                name_one = f'ENG{run}'\n",
    "                name_two = f'ENG{run}'\n",
    "            else:\n",
    "                name_one = f'ENG00{run}'\n",
    "                name_two = f'ENGINX00{run}'\n",
    "            ext_one = '.raw'\n",
    "            ext_two = '.nxs'\n",
    "            testdir_one = f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_one}{ext_one}'\n",
    "            testdir_two = f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_two}{ext_one}'\n",
    "            testdir_three = f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_one}{ext_two}'\n",
    "            testdir_four = f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_two}{ext_two}'\n",
    "            check_one = pathlib.Path(testdir_one)\n",
    "            check_two = pathlib.Path(testdir_two)\n",
    "            check_three = pathlib.Path(testdir_three)\n",
    "            check_four = pathlib.Path(testdir_four)\n",
    "            if check_one.exists():\n",
    "                dire = testdir_one\n",
    "                name = f'ENGINX00{run}'\n",
    "                dSpace = f'{name}-dSpacing'\n",
    "                sumSpec = f'{name}-sumSpec'\n",
    "                try:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_One, SpectrumMax=sMax_One)\n",
    "                except ValueError:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_Two, SpectrumMax=sMax_Two)\n",
    "                ConvertUnits(InputWorkspace=name, OutputWorkspace=dSpace, Target='dSpacing', AlignBins=True)\n",
    "                SumSpectra(InputWorkspace=dSpace, OutputWorkspace=sumSpec)\n",
    "                try:\n",
    "                    temp = mtd[sumSpec].getRun().getLogData(app).filtered_value\n",
    "                except RuntimeError:\n",
    "                    print(f'Unable to find {app} in file.')\n",
    "                    try:\n",
    "                        temp = np.loadtxt(fname=f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_one}_{app}.txt',dtype='float',usecols=(1))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f'Unable to find {app} file in folder.')\n",
    "                        temp = 0\n",
    "                except UnboundLocalError:\n",
    "                    temp = 0\n",
    "                if isinstance(temp, (np.ndarray, list)) == True:\n",
    "                    try:\n",
    "                        temp = temp[-1]\n",
    "                        temp = round(temp,2)\n",
    "                    except IndexError:\n",
    "                        temp = temp\n",
    "                        temp = round(float(temp),2)\n",
    "                else:\n",
    "                    temp = temp\n",
    "                    temp = round(float(temp),2)\n",
    "                return dire, temp, sumSpec\n",
    "            elif check_two.exists():\n",
    "                dire = testdir_two\n",
    "                name = f'ENGINX00{run}'\n",
    "                dSpace = f'{name}-dSpacing'\n",
    "                sumSpec = f'{name}-sumSpec'\n",
    "                try:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_One, SpectrumMax=sMax_One)\n",
    "                except ValueError:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_Two, SpectrumMax=sMax_Two)\n",
    "                ConvertUnits(InputWorkspace=name, OutputWorkspace=dSpace, Target='dSpacing', AlignBins=True)\n",
    "                SumSpectra(InputWorkspace=dSpace, OutputWorkspace=sumSpec)\n",
    "                try:\n",
    "                    temp = mtd[sumSpec].getRun().getLogData(app).filtered_value\n",
    "                except RuntimeError:\n",
    "                    print(f'Unable to find {app} in file.')\n",
    "                    try:\n",
    "                        temp = np.loadtxt(fname=f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_two}_{app}.txt',dtype='float',usecols=(1))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f'Unable to find {app} file in folder.')\n",
    "                        temp = 0\n",
    "                except UnboundLocalError:\n",
    "                    temp = 0\n",
    "                if isinstance(temp, (np.ndarray, list)) == True:\n",
    "                    try:\n",
    "                        temp = temp[-1]\n",
    "                        temp = round(temp,2)\n",
    "                    except IndexError:\n",
    "                        temp = temp\n",
    "                        temp = round(float(temp),2)\n",
    "                else:\n",
    "                    temp = temp\n",
    "                    temp = round(float(temp),2)\n",
    "                return dire, temp, sumSpec\n",
    "            elif check_three.exists():\n",
    "                dire = testdir_three\n",
    "                name = f'ENGINX00{run}'\n",
    "                dSpace = f'{name}-dSpacing'\n",
    "                sumSpec = f'{name}-sumSpec'\n",
    "                try:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_One, SpectrumMax=sMax_One)\n",
    "                except ValueError:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_Two, SpectrumMax=sMax_Two)\n",
    "                ConvertUnits(InputWorkspace=name, OutputWorkspace=dSpace, Target='dSpacing', AlignBins=True)\n",
    "                SumSpectra(InputWorkspace=dSpace, OutputWorkspace=sumSpec)\n",
    "                try:\n",
    "                    temp = mtd[sumSpec].getRun().getLogData(app).filtered_value\n",
    "                except RuntimeError:\n",
    "                    print(f'Unable to find {app} in file.')\n",
    "                    try:\n",
    "                        temp = np.loadtxt(fname=f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_one}_{app}.txt',dtype='float',usecols=(1))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f'Unable to find {app} file in folder.')\n",
    "                        temp = 0\n",
    "                except UnboundLocalError:\n",
    "                    temp = 0\n",
    "                if isinstance(temp, (np.ndarray, list)) == True:\n",
    "                    try:\n",
    "                        temp = temp[-1]\n",
    "                        temp = round(temp,2)\n",
    "                    except IndexError:\n",
    "                        temp = temp\n",
    "                        temp = round(float(temp),2)\n",
    "                else:\n",
    "                    temp = temp\n",
    "                    temp = round(float(temp),2)\n",
    "                return dire, temp, sumSpec\n",
    "            elif check_four.exists():\n",
    "                dire = testdir_four\n",
    "                name = f'ENGINX00{run}'\n",
    "                dSpace = f'{name}-dSpacing'\n",
    "                sumSpec = f'{name}-sumSpec'\n",
    "                try:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_One, SpectrumMax=sMax_One)\n",
    "                except ValueError:\n",
    "                    Load(Filename=dire, OutputWorkspace=name, SpectrumMin=sMin_Two, SpectrumMax=sMax_Two)\n",
    "                ConvertUnits(InputWorkspace=name, OutputWorkspace=dSpace, Target='dSpacing', AlignBins=True)\n",
    "                SumSpectra(InputWorkspace=dSpace, OutputWorkspace=sumSpec)\n",
    "                try:\n",
    "                    temp = mtd[sumSpec].getRun().getLogData(app).filtered_value\n",
    "                except RuntimeError:\n",
    "                    print(f'Unable to find {app} in file.')\n",
    "                    try:\n",
    "                        temp = np.loadtxt(fname=f'//isis/inst$/NDXENGINX/Instrument/data/cycle_{year}_{cycle}/{name_two}_{app}.txt',dtype='float',usecols=(1))\n",
    "                    except FileNotFoundError:\n",
    "                        print(f'Unable to find {app} file in folder.')\n",
    "                        temp = 0\n",
    "                except UnboundLocalError:\n",
    "                    temp = 0\n",
    "                if isinstance(temp, (np.ndarray, list)) == True:\n",
    "                    try:\n",
    "                        temp = temp[-1]\n",
    "                        temp = round(temp,2)\n",
    "                    except IndexError:\n",
    "                        temp = temp\n",
    "                        temp = round(float(temp),2)\n",
    "                else:\n",
    "                    temp = temp\n",
    "                    temp = round(float(temp),2)\n",
    "                return dire, temp, sumSpec\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c790b-76fb-41ca-a588-c2dff16560a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PaBrun = 324889\n",
    "#Testing the code with Niobium Runs\n",
    "#PaBrun = 351364\n",
    "#PaBrun = 351464\n",
    "#PaBrun = 351538\n",
    "#PaBrun = 351637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b3d4b-6794-4e84-9622-a812464b2554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit as lm\n",
    "\n",
    "def gaussian_function(x, a, x0, s,b):\n",
    "    return b+a*np.exp(-(x-x0)**2/(2*s**2))\n",
    "   \n",
    "def residualGauss(pars,x,data):\n",
    "    model = gaussian_function(x, a=pars['a'],x0=pars['x0'],s=pars['s'],b=pars['b']) \n",
    "    return model - data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f64a6-f9ad-4088-b07c-3060f36eca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are all the dependencies for the Cubic Peak Fit. Other functions that had to be brought in to make it work\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx]\n",
    "\n",
    "def find_nearest_ind(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def logData(x,y):\n",
    "    yNorm = y/np.amax(y)\n",
    "    xLog = np.log(x)\n",
    "    inter = (x[-1] - x[0])/len(x)\n",
    "    xSize = np.arange(xLog[0],xLog[-1],inter)\n",
    "    xLogInterp = np.interp(xSize,x,xLog)\n",
    "    yLogInterp = np.interp(xSize,xLog,yNorm)\n",
    "    return xSize,yLogInterp,inter\n",
    "\n",
    "def logInter(run,datx,bins):    \n",
    "    _,temp,sumSpec = fileSearch(run,exp=None,bank=None)\n",
    "    y = mtd[sumSpec].dataY(0)\n",
    "    x = mtd[sumSpec].dataX(0)[0:len(y)]\n",
    "    if x[0] < datx[0]:\n",
    "        x_datx_ind = find_nearest_ind(x, datx[0])\n",
    "#        print(x_datx_ind)\n",
    "        x = x[x_datx_ind:list(x).index(x[-1])]\n",
    "        y = y[x_datx_ind:len(y)]\n",
    "    if len(y) < len(datx):\n",
    "        diff_x = int(abs(len(datx) - len(x))/2)\n",
    "        diff_y = int(abs(len(datx) - len(y))/2)\n",
    "        y = np.pad(y, (diff_y,diff_y), 'constant')\n",
    "        x = np.pad(x, (diff_x,diff_x), 'linear_ramp',end_values=(datx[0],datx[-1]))\n",
    "    if len(x) < len(y):\n",
    "        dist = len(y) - len(x)\n",
    "        inval = x[-1]/len(datx)\n",
    "        x = np.pad(x, (0,dist), 'linear_ramp',end_values=(datx[0],(datx[-1]+(dist*inval))))\n",
    "    if len(x) > len(y):\n",
    "        dist = len(x) - len(y)\n",
    "        y = np.pad(y, (0,dist), 'constant')\n",
    "#    print(len(datx),len(x),len(y))\n",
    "    yNorm = y/np.amax(y)\n",
    "    xLog = np.log(x)\n",
    "    xSize = np.arange(xLog[0],xLog[-1],bins)\n",
    "#    print(len(xSize),len(xLog),len(yNorm))\n",
    "    xLogInterp = np.interp(xSize,x,xLog)\n",
    "    yLogInterp = np.interp(xSize,xLog,yNorm)\n",
    "    return x,y,xSize,yLogInterp,temp\n",
    "\n",
    "def noise_gen(level,size):\n",
    "    noi_arr = np.random.normal(0,level,size)\n",
    "    #print(noi_arr)\n",
    "    for i in range(len(noi_arr)):\n",
    "        if noi_arr[i] < 0:\n",
    "            noi_arr[i] = 0\n",
    "    return noi_arr\n",
    "    \n",
    "def CIFData(element,noise_lvl,size,drange=[0.67,3.36],plot_CIF=True):\n",
    "    #This is to create x values for the data\n",
    "    x = np.linspace(drange[0],drange[1],size)\n",
    "    plot = np.zeros(len(x),)\n",
    "    #The bank distance and source distance are from the ENGIN-X manual in order to replicate, as much as possible,\n",
    "    #realistic data\n",
    "    ws = CreateSampleWorkspace(Function='Multiple Peaks',BankDistanceFromSample=0.01,SourceDistanceFromSample=50)\n",
    "    LoadCIF(ws,f'C:/Users/ynn46697/Documents/ISIS/Grain_Analysis/Periodic_Table_CIFs/{element}.cif')\n",
    "    sample = ws.sample().getCrystalStructure()\n",
    "    unitCell = sample.getUnitCell()\n",
    "    a = unitCell.a()\n",
    "    pg = sample.getSpaceGroup().getPointGroup()\n",
    "    generator = ReflectionGenerator(sample)\n",
    "    hkls = list(generator.getUniqueHKLs(drange[0],drange[-1]))\n",
    "    if plot_CIF == False:\n",
    "        #Certain functions only need the CIF Data generation as a shortcut to determining hkls\n",
    "        return hkls\n",
    "    else:\n",
    "        pass\n",
    "    peaks = list(generator.getDValues(hkls))\n",
    "    Fsq = list(generator.getFsSquared(hkls))\n",
    "    j = list(map(lambda x : len(pg.getEquivalents(x)), hkls))\n",
    "    s = list(map(lambda x : x*0.005, peaks))\n",
    "    p = []\n",
    "    fl = []\n",
    "    height = []\n",
    "    for i in range(len(peaks)):\n",
    "        p.append((j[i]*Fsq[i]*(peaks[i]**4))/(a**3)) #This equation comes from the Introduction to \n",
    "        #the Characterization of Residual Stress by Neutron Diffraction book by M.T. Hutchings, P.J. Withers, et al.\n",
    "        #with some parameters missing as these are undeterminable or negligible without a real experiment\n",
    "        fl.append(flux[find_nearest_ind(flux_pt,peaks[i])])\n",
    "        height.append(fl[i] * p[i])\n",
    "    for peak in peaks:\n",
    "        height_val = height[peaks.index(peak)]\n",
    "        s_val = s[peaks.index(peak)]\n",
    "        b = 0\n",
    "        plot += gaussian_function(x,height_val,peak,s_val,b)\n",
    "    noise = noise_gen((np.amax(plot)*noise_lvl),len(plot))\n",
    "    plot = plot + noise\n",
    "    return x,plot,hkls\n",
    "\n",
    "def bg_fit(x,y,loop=25):\n",
    "    #This uses chebyshev polynomials to fit to the background\n",
    "    cheb_fit,_ = cheb.fit(x,y,0,full=True)\n",
    "    coef = np.sum(cheb_fit.coef)\n",
    "    for i in range(1,loop):\n",
    "        cheb_fit,params = cheb.fit(x,y,i,full=True)\n",
    "        new_coef = np.sum(cheb_fit.coef)\n",
    "        coef_check = abs(0 - coef)\n",
    "        new_check = abs(0 - new_coef)\n",
    "        if new_check < coef_check:\n",
    "            coef = new_coef\n",
    "            best_fit = cheb_fit\n",
    "        else:\n",
    "            pass\n",
    "    bg_x,bg_y = best_fit.linspace(n=len(x)) #This can be used to add a background fit to peak fits\n",
    "    y_hat = abs(y - bg_y) #This can be used to remove the background from particularly noisy data\n",
    "    return bg_x,bg_y,y_hat\n",
    "\n",
    "_,_,sumSpec = fileSearch(358221,exp=None,bank=None)\n",
    "\n",
    "flux = mtd[sumSpec].dataY(0)\n",
    "flux_pt = mtd[sumSpec].dataX(0)[0:len(flux)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cf37c4-8b3c-4dda-8c21-d88e77974953",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This fits peaks for fcc and bcc\n",
    "def cubicPeakFit(run,exp,bank,size,bg,peak_bg):\n",
    "    paramsArr = []\n",
    "    #First it generates a Vanadium and Copper sample then logs their x values\n",
    "    bcc_x,bcc_y,bcc_hkls = CIFData('V',0.005,size)\n",
    "    fcc_x,fcc_y,fcc_hkls = CIFData('Cu',0.005,size)\n",
    "    bcc_xS,bcc_yLog,bcc_inter = logData(bcc_x,bcc_y)\n",
    "    fcc_xS,fcc_yLog,fcc_inter = logData(fcc_x,fcc_y)\n",
    "    #After this, it retrieves the run and logs the x values of its data too\n",
    "    x,y,dat_xS,dat_yLog,temp = logInter(run,bcc_x,max(bcc_inter,fcc_inter))\n",
    "    bg_x,bg_y,y_hat = bg_fit(x,y,loop=25)\n",
    "\n",
    "    #This determines the furthest peak in the dataset\n",
    "    lim = 1\n",
    "    dist = 0.025\n",
    "    rep = int(lim / dist)\n",
    "    for i in range(rep):\n",
    "        dat_end = int(len(x)*lim)\n",
    "        dat_break = int(len(x)*(lim-dist))\n",
    "        dat_trunx = x[dat_break:dat_end]\n",
    "        dat_trunc = y[dat_break:dat_end]\n",
    "        dat_peaks,dat_params = fp(dat_trunc,prominence=15,distance=100)\n",
    "        lim = lim - dist\n",
    "        if len(dat_peaks) > 0:\n",
    "            break\n",
    "    dat_peaks,_ = fp(dat_trunc)\n",
    "    dat_yPeaks = list(dat_trunc[dat_peaks])\n",
    "    dat_x0 = list(dat_trunx[dat_peaks])\n",
    "    dat_far = np.amax(dat_yPeaks) #This is the furthest peak in y\n",
    "    dat_fax = dat_x0[dat_yPeaks.index(dat_far)] #And then in x\n",
    "    #The data is then correlated against both the bcc and fcc data\n",
    "    corr_bcc = np.correlate(bcc_yLog,dat_yLog,mode='full')\n",
    "    corr_fcc = np.correlate(fcc_yLog,dat_yLog,mode='full')\n",
    "    #Then the peaks of the correlation are identified\n",
    "    corr_axes = np.linspace(0,len(corr_bcc),len(corr_bcc))\n",
    "    corr_bcc_peaks,corr_bcc_params = fp(corr_bcc,prominence=1)\n",
    "    corr_fcc_peaks,corr_fcc_params = fp(corr_fcc,prominence=1)\n",
    "    #These peaks are then collated\n",
    "    bcc_x = corr_axes[corr_bcc_peaks]\n",
    "    bcc_y = corr_bcc[corr_bcc_peaks]\n",
    "    fcc_x = corr_axes[corr_fcc_peaks]\n",
    "    fcc_y = corr_fcc[corr_fcc_peaks]\n",
    "    #And sorted\n",
    "    bcc_sort = sorted(bcc_y)\n",
    "    fcc_sort = sorted(fcc_y)\n",
    "    #Then the difference between the two highest peaks within the prominence are determined\n",
    "    bcc_peak_diff = bcc_sort[-1]-bcc_sort[-2]\n",
    "    fcc_peak_diff = fcc_sort[-1]-fcc_sort[-2]\n",
    "\n",
    "    #The second peak of each crystal structure are then estimated in case the correlation failed\n",
    "    a_bcc = dat_fax*np.sqrt(2)\n",
    "    a_fcc = dat_fax*np.sqrt(3)\n",
    "    d_two_bcc = a_bcc / 2\n",
    "    d_two_fcc = a_fcc / 2\n",
    "    loc_two_bcc = find_nearest_ind(x,d_two_bcc)\n",
    "    loc_two_fcc = find_nearest_ind(x,d_two_fcc)\n",
    "    range_two_bcc = range(loc_two_bcc-200,loc_two_bcc+200)\n",
    "    range_two_fcc = range(loc_two_fcc-200,loc_two_fcc+200)\n",
    "    bcc_two_peak,bcc_two_par = fp(y[range_two_bcc],prominence=15)\n",
    "    fcc_two_peak,fcc_two_par = fp(y[range_two_fcc],prominence=15)\n",
    "    \n",
    "    if bcc_peak_diff > fcc_peak_diff:\n",
    "        #This is a double check because sometimes the dataset reads as the wrong crystal structure\n",
    "        if y[loc_two_fcc] > y[loc_two_bcc]:\n",
    "            #This is a triple check\n",
    "            if len(bcc_two_par['prominences']) == len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            elif len(bcc_two_par['prominences']) > len(fcc_two_par['prominences']):\n",
    "                cryst = 'bcc'\n",
    "                hkls = bcc_hkls\n",
    "                a = a_bcc\n",
    "            elif len(bcc_two_par['prominences']) < len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            else:\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "        else:\n",
    "            if len(bcc_two_par['prominences']) == len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            elif len(bcc_two_par['prominences']) > len(fcc_two_par['prominences']):\n",
    "                cryst = 'bcc'\n",
    "                hkls = bcc_hkls\n",
    "                a = a_bcc\n",
    "            elif len(bcc_two_par['prominences']) < len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            else:\n",
    "                cryst = 'bcc'\n",
    "                hkls = bcc_hkls\n",
    "                a = a_bcc\n",
    "    else:\n",
    "        if y[loc_two_bcc] > y[loc_two_fcc]:\n",
    "            if len(bcc_two_par['prominences']) == len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            elif len(bcc_two_par['prominences']) > len(fcc_two_par['prominences']):\n",
    "                cryst = 'bcc'\n",
    "                hkls = bcc_hkls\n",
    "                a = a_bcc\n",
    "            elif len(bcc_two_par['prominences']) < len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            else:\n",
    "                cryst = 'bcc'\n",
    "                hkls = bcc_hkls\n",
    "                a = a_bcc\n",
    "        else:\n",
    "            if len(bcc_two_par['prominences']) == len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            elif len(bcc_two_par['prominences']) > len(fcc_two_par['prominences']):\n",
    "                cryst = 'bcc'\n",
    "                hkls = bcc_hkls\n",
    "                a = a_bcc\n",
    "            elif len(bcc_two_par['prominences']) < len(fcc_two_par['prominences']):\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "            else:\n",
    "                cryst = 'fcc'\n",
    "                hkls = fcc_hkls\n",
    "                a = a_fcc\n",
    "    #This was for testing, to make sure it's reading the correct peak, crystal structure, and lattice parameter            \n",
    "    check_arr = [dat_fax,cryst,a]\n",
    "\n",
    "    x0 = []\n",
    "    x0_idx = []\n",
    "    hkl_arr = []\n",
    "    #print(hkls)\n",
    "    for hkl in hkls:\n",
    "        h = hkl[0]\n",
    "        k = hkl[1]\n",
    "        l = hkl[2]\n",
    "        #For each hkl, this calculates a peak then adds it to an array\n",
    "        peak = a / np.sqrt(h**2+k**2+l**2)\n",
    "        x0_ind = find_nearest_ind(x, peak)\n",
    "        #print(peak,x0_ind)\n",
    "        #print(find_nearest(x,peak))\n",
    "        if x0_ind not in x0_idx:\n",
    "            hkl_arr.append([f'{int(h)}{int(k)}{int(l)}'])\n",
    "            x0.append(peak)\n",
    "            x0_idx.append(x0_ind)\n",
    "        else:\n",
    "            pass\n",
    "    plot = np.zeros(len(x),)\n",
    "    for peak in x0:\n",
    "        #This array is then used to perform the peak fitting as before\n",
    "        peak_hkl = hkl_arr[x0.index(peak)]\n",
    "        height = y[find_nearest_ind(x, peak)]\n",
    "        #Due to the nature of these peaks being calculated not found, this finds the nearest approximation within\n",
    "        #the data\n",
    "        xOne = x[find_nearest_ind(x, peak)-75:find_nearest_ind(x, peak)]\n",
    "        xTwo = x[find_nearest_ind(x, peak)+1:find_nearest_ind(x, peak)+75]\n",
    "        yOne = y[find_nearest_ind(x, peak)-75:find_nearest_ind(x, peak)]\n",
    "        yTwo = y[find_nearest_ind(x, peak)+1:find_nearest_ind(x, peak)+75]\n",
    "        xOneArr = xOne[list(np.argwhere(yOne <= height/2))]\n",
    "        xTwoArr = xTwo[list(np.argwhere(yTwo <= height/2))]\n",
    "        try:\n",
    "            x1 = np.amax(xOneArr)\n",
    "            x2 = np.amin(xTwoArr)\n",
    "            fwhm = x2 - x1\n",
    "            sigma = fwhm / (2 * m.sqrt(2 * m.log(2)))\n",
    "        except ValueError:\n",
    "            #Failsafe if the code can't calculate fwhm\n",
    "            sigma = 0.005 * peak\n",
    "        if peak_bg == True:\n",
    "            backg = bg_y\n",
    "        else:\n",
    "            backg = 0\n",
    "        peak_ws = CreateSampleWorkspace()\n",
    "        #Rather than lmfit, this function uses PlotPeakByLogValue as this function is much more rigid with the x0 values\n",
    "        PeakFitString = f\"name=Gaussian,Height={height},PeakCentre={peak},Sigma={sigma}\"\n",
    "        PlotPeakByLogValue(Input=peak_ws, OutputWorkspace='PeakFit', Function=PeakFitString, FitType='Individual')\n",
    "        fit_res = mtd['PeakFit'].toDict()\n",
    "        height_fit = fit_res['Height'][0]\n",
    "        peak_fit = fit_res['PeakCentre'][0]\n",
    "        sig_fit = fit_res['Sigma'][0]\n",
    "        ii_fit = fit_res['Integrated Intensity'][0]\n",
    "        para = [temp,peak_hkl,peak,height_fit,sig_fit,bg_y[find_nearest_ind(x, peak)],ii_fit]\n",
    "        paramsArr.append(para)\n",
    "        best_fit = gaussian_function(x=x, a=height_fit, x0=peak_fit, s=sig_fit,b=backg)\n",
    "        if x0.index(peak) > 0:\n",
    "            for j in range(len(plot)):\n",
    "                check = [best_fit[j],plot[j]]\n",
    "                bestCheck = np.amax(check)\n",
    "                plot[j] = bestCheck\n",
    "        else:\n",
    "            plot = best_fit\n",
    "    if bg == True:\n",
    "        y = y_hat\n",
    "    else:\n",
    "        pass\n",
    "    #print(x0_idx)\n",
    "    return temp, x, y, plot, paramsArr\n",
    "\n",
    "#This takes a list of peak locations provided by the user and fits peaks to it\n",
    "def inputPeakFit(run,exp,bank,x0,cryst,size,bg,peak_bg):\n",
    "    x0.sort()\n",
    "    filepath, temp, sumSpec = fileSearch(run,exp,bank)\n",
    "    y = mtd[sumSpec].dataY(0)\n",
    "    x = mtd[sumSpec].dataX(0)[0:len(y)]\n",
    "    bg_x,bg_y,y_hat = bg_fit(x,y,loop=25)\n",
    "    x = list(x)\n",
    "    x0_ind = []\n",
    "    yPeak = []\n",
    "    plot = []\n",
    "    for i in range(len(x0)):\n",
    "        #This makes sure the x0 values are in the x data\n",
    "        x0_new = find_nearest(x, x0[i])\n",
    "        x0[i] = x0_new\n",
    "        x0_ind.append(x.index(x0[i]))\n",
    "    for ind in x0_ind:\n",
    "        #This searches the range around x0 in the y axis to make sure the x0 determined fits\n",
    "        lb = ind - 100\n",
    "        ub = ind + 100\n",
    "        x0_range = list(range(lb,ub))\n",
    "        yPeak_range = y[x0_range]\n",
    "        yPeak.append(np.amax(yPeak_range))\n",
    "    y = list(y)\n",
    "    for i in range(len(yPeak)):\n",
    "        #This matches the x0 back to the y\n",
    "        peak_ind = y.index(yPeak[i])\n",
    "        x0[i] = x[peak_ind]\n",
    "        \n",
    "    x0_rat = x0[-1]/x0[-2]\n",
    "    if cryst == None:\n",
    "        #If the user doesn't provide a crystal structure, this uses the ratios of the first and second peak's\n",
    "        #location to determine which structure it may be\n",
    "        cryst_arr = ['scc','bcc','fcc','fcc_dia']\n",
    "        scc_rat = np.sqrt(2)/np.sqrt(1)\n",
    "        bcc_rat = np.sqrt(4)/np.sqrt(2)\n",
    "        fcc_rat = np.sqrt(4)/np.sqrt(3)\n",
    "        fccdia_rat = np.sqrt(8)/np.sqrt(3)\n",
    "        rats_arr = [abs(x0_rat-scc_rat),abs(x0_rat-bcc_rat),abs(x0_rat-fcc_rat),abs(x0_rat-fccdia_rat)]\n",
    "        rats_best = np.amin(rats_arr)\n",
    "        cryst = cryst_arr[rats_arr.index(rats_best)]\n",
    "    if cryst == 'fcc':\n",
    "        hkls = CIFData('Cu',0.005,size,drange=[0.01,x[-1]],plot_CIF=False)\n",
    "    elif cryst == 'bcc':\n",
    "        hkls = CIFData('V',0.005,size,drange=[0.01,x[-1]],plot_CIF=False)\n",
    "    elif cryst == 'scc':\n",
    "        hkls = CIFData('Po',0.005,size,drange=[0.01,x[-1]],plot_CIF=False)\n",
    "    elif cryst == 'fcc_dia':\n",
    "        hkls = CIFData('Sn',0.005,size,drange=[0.01,x[-1]],plot_CIF=False)\n",
    "    else:\n",
    "        hkls = CIFData('Po',0.005,size,drange=[0.01,x[-1]],plot_CIF=False)\n",
    "    paramsArr = []\n",
    "    x0_check = []\n",
    "    for peak in x0:\n",
    "        x0_idx = x.index(peak)\n",
    "        #This is to prevent nearby peaks being repeated as the same hkl\n",
    "        if x0_idx in x0_check:\n",
    "            pass\n",
    "        else:\n",
    "            x0_check.append(x0_idx)\n",
    "            try:\n",
    "                peak_hkl = hkls[x0.index(peak)]\n",
    "            except IndexError:\n",
    "                #If the CIF Data can't find a hkl for the selected peak, this allows the code to continue\n",
    "                peak_hkl = [0,0,0]\n",
    "            height = y[x0_idx]\n",
    "            xOne = x[x0_idx-75:x0_idx]\n",
    "            xTwo = x[x0_idx+1:x0_idx+75]\n",
    "            yOne = y[x0_idx-75:x0_idx]\n",
    "            yTwo = y[x0_idx+1:x0_idx+75]\n",
    "            xOne = np.array(xOne)\n",
    "            xTwo = np.array(xTwo)\n",
    "            xOneArr = xOne[list(np.argwhere(yOne <= height/2))]\n",
    "            xTwoArr = xTwo[list(np.argwhere(yTwo <= height/2))]\n",
    "            try:\n",
    "                x1 = np.amax(xOneArr)\n",
    "                x2 = np.amin(xTwoArr)\n",
    "                fwhm = x2 - x1\n",
    "                sigma = fwhm / (2 * m.sqrt(2 * m.log(2)))\n",
    "            except ValueError:\n",
    "                sigma = 0.05 * peak\n",
    "            if peak_bg == True:\n",
    "                backg = bg_y\n",
    "            else:\n",
    "                backg = 0\n",
    "            peak_ws = CreateSampleWorkspace()\n",
    "            #Again using PlotByPeakLogValue as the user already knows where the x0 is\n",
    "            PeakFitString = f\"name=Gaussian,Height={height},PeakCentre={peak},Sigma={sigma}\"\n",
    "            PlotPeakByLogValue(Input=peak_ws, OutputWorkspace='PeakFit', Function=PeakFitString, FitType='Individual')\n",
    "            fit_res = mtd['PeakFit'].toDict()\n",
    "            height_fit = fit_res['Height'][0]\n",
    "            peak_fit = fit_res['PeakCentre'][0]\n",
    "            sig_fit = fit_res['Sigma'][0]\n",
    "            ii_fit = fit_res['Integrated Intensity'][0]\n",
    "            para = [temp,peak_hkl,peak,height_fit,sig_fit,bg_y[find_nearest_ind(x, peak)],ii_fit]\n",
    "            paramsArr.append(para)\n",
    "            x = np.array(x)\n",
    "            best_fit = gaussian_function(x=x, a=height_fit, x0=peak_fit, s=sig_fit,b=backg)\n",
    "            if x0.index(peak) == 0:\n",
    "                plot = best_fit\n",
    "            else:\n",
    "                for i in range(len(plot)):\n",
    "                    check = np.amax([plot[i],best_fit[i]])\n",
    "                    plot[i] = check\n",
    "            x = list(x)\n",
    "        if bg == True:\n",
    "            y = y_hat\n",
    "        else:\n",
    "            pass\n",
    "    print(x0_check)\n",
    "    return temp, x, y, plot, paramsArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fba5246-8b87-40d0-b5ff-b446931ab457",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PeakAcrossBanks with the peaks found by Scipy.signal and the fit done by mantid\n",
    "#This is the PaB cell that runs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from scipy.signal import find_peaks as fp\n",
    "import math as m\n",
    "\n",
    "import pandas as pd\n",
    "from itables import init_notebook_mode, options\n",
    "init_notebook_mode(all_interactive=True)\n",
    "options.maxBytes = 0\n",
    "\n",
    "#from ipyaggrid import Grid\n",
    "\n",
    "def PeakAcrossBanks(RunNo,peak_list=None,PixelGrouping=None,RebinParams='-0.0005'):\n",
    "    # Load and focus data\n",
    "    #filepath = fileSearch(str(RunNo))\n",
    "    filepath,_,sumSpec = fileSearch(run=RunNo,exp=None,bank=None)\n",
    "    testX = mtd[sumSpec].dataX(0)\n",
    "    DiffnDataWS = Load(Filename=filepath, OutputWorkspace='DiffnDataWS', SpectrumMax=2400)\n",
    "    ConvertUnits(InputWorkspace='DiffnDataWS', OutputWorkspace='DiffnDataWS-dSp', Target='dSpacing', AlignBins=True)\n",
    "    # Group pixels and d-spacing axis to appropriate bin sizes\n",
    "    LoadDetectorsGroupingFile(InputFile=PixelGrouping, InputWorkspace='DiffnDataWS-dSp', OutputWorkspace='XML-SpatialGrouping')\n",
    "    GroupDetectors(InputWorkspace='DiffnDataWS-dSp', OutputWorkspace='DiffnDataWS-dSp-grp', IgnoreGroupNumber=False, CopyGroupingFromWorkspace='XML-SpatialGrouping')\n",
    "    Rebin(InputWorkspace='DiffnDataWS-dSp-grp', OutputWorkspace='DiffnDataWS-dSp-grp-reb', Params=RebinParams)\n",
    "    # Fit peak(s) to all spectra in workspace\n",
    "    NumSpectra = mtd['DiffnDataWS-dSp-grp-reb'].getNumberHistograms()\n",
    "    SpectraRefString = \";\".join([f'DiffnDataWS-dSp-grp-reb,sp{i+1}' for i in range(NumSpectra)])\n",
    "    #peaklen, peaklist, x0, x, y = peakFit(RunNo)\n",
    "    #hkls,a,X0,struc = HKL_Calc(x0)\n",
    "    if peak_list == None:\n",
    "        _, x, y, plot, paramsArr = cubicPeakFit(run=RunNo,exp=None,bank=None,size=len(testX),bg=False,peak_bg=True)\n",
    "    else:\n",
    "        _, x, y, plot, paramsArr = inputPeakFit(run=RunNo,exp=None,bank=None,x0=peak_list,cryst=None,size=len(testX),bg=False,peak_bg=True)\n",
    "    x0 = []\n",
    "    hkls = []\n",
    "    for para in paramsArr:\n",
    "        x0.append(para[2])\n",
    "        hkls.append(para[1])\n",
    "    #print(hkls)\n",
    "    #print(x0)\n",
    "    peaklen = len(x0)\n",
    "    #print(peaklen)\n",
    "    #print(hkls)\n",
    "    dMin, dMax = np.amin(x), np.amax(x)\n",
    "    FitResults=None # Merge\n",
    "    for i in range(peaklen):\n",
    "        PeakPosition = x0[i]\n",
    "        #print(PeakPosition)\n",
    "        if dMin < PeakPosition < dMax:\n",
    "            PeakHeights=[]\n",
    "            for j in range(NumSpectra):\n",
    "                PeakHeights.append(mtd['DiffnDataWS-dSp-grp-reb'].dataY(j)[mtd['DiffnDataWS-dSp-grp-reb'].yIndexOfX(PeakPosition)])\n",
    "            PeakHeight = np.mean(PeakHeights)\n",
    "            #print(f'Height = {PeakHeight}')\n",
    "            #Add width\n",
    "            #width = findSigma(PeakPosition,x,y)\n",
    "            #PeakFitString = f\"name=Gaussian,Height={PeakHeight},PeakCentre={PeakPosition},Sigma={width}\"\n",
    "            #Calculating sigma seemed to create a lot of NaNs for some reason\n",
    "            PeakFitString = f\"name=Gaussian,Height={PeakHeight},PeakCentre={PeakPosition},Sigma={PeakPosition*0.005}\"\n",
    "            PlotPeakByLogValue(Input=SpectraRefString, OutputWorkspace='DiffnDataWS-dSp-grp-reb-Fit', Function=PeakFitString, FitType='Individual', \n",
    "                               StartX=PeakPosition*0.98, EndX=PeakPosition*1.02)\n",
    "            # Convert results from mantid TableWorkspace to Pandas, and add extra info\n",
    "            ResultsDict = mtd['DiffnDataWS-dSp-grp-reb-Fit'].toDict()\n",
    "            #print(ResultsDict)\n",
    "            ResultsDict['GroupNo'] = ResultsDict.pop('axis-1')\n",
    "            # Rename column headings to include peak name - could use MultiIndex for this?\n",
    "            for ColumnName in ['Height', 'Height_Err', 'PeakCentre', 'PeakCentre_Err', 'Sigma', 'Sigma_Err', 'Integrated Intensity', 'Integrated Intensity_Err', 'Chi_squared']:\n",
    "                k = i + 1\n",
    "                #ResultsDict[f'Peak No.{k}-{ColumnName}'] = ResultsDict.pop(ColumnName)\n",
    "                ResultsDict[f'{hkls[i]}-{ColumnName}'] = ResultsDict.pop(ColumnName)\n",
    "            ResultsDF = pd.DataFrame.from_dict(ResultsDict).set_index(\"GroupNo\")\n",
    "            \n",
    "            if FitResults is not None:\n",
    "                FitResults = pd.merge(FitResults, ResultsDF, on=\"GroupNo\") # , index=ResultsDict['GroupNo']\n",
    "            else:\n",
    "                FitResults = ResultsDF\n",
    "    #print(ResultsDict.keys())\n",
    "    dfHDF = FitResults\n",
    "    #print(type(FitResults))\n",
    "    dfHDF.reset_index(drop=True)\n",
    "\n",
    "    #print(FitResults.keys())\n",
    "    name = f'{RunNo}_Grp_HDF'\n",
    "    #print(FitResults.keys())\n",
    "    try:\n",
    "        storeGrpPeaks(name, FitResults, peaklen,hkls)\n",
    "    except ValueError:\n",
    "        pass\n",
    "    #storeGrpPeaks(name,FitResults,peaklen)\n",
    "\n",
    "    Azimuths = [np.rad2deg(mtd['DiffnDataWS-dSp-grp-reb'].spectrumInfo().azimuthal(i)) for i in range(NumSpectra)]\n",
    "    TwoThetas = [np.rad2deg(mtd['DiffnDataWS-dSp-grp-reb'].spectrumInfo().signedTwoTheta(i)) for i in range(NumSpectra)]\n",
    "    #Angles= [np.rad2deg(mtd['DiffnDataWS-dSp-grp-reb'].spectrumInfo().geographicalAngles(i)) for i in range(NumSpectra)]\n",
    "    FitResults['Azimuth']=Azimuths\n",
    "    FitResults['TwoTheta']=TwoThetas\n",
    "    #FitResults['Angles']=Angles\n",
    "    for PixelGroup in FitResults.index:\n",
    "        # Fix issues with signs and degenerate values in the angle data\n",
    "        if -90 < FitResults.loc[PixelGroup, \"Azimuth\"] < 90:   # Indicates north bank\n",
    "            FitResults.loc[PixelGroup, \"TwoTheta\"] = abs(FitResults.loc[PixelGroup, \"TwoTheta\"])\n",
    "        if FitResults.loc[PixelGroup, \"Azimuth\"] < -90:        # -163 or -171.5, corresponds to -17 or -8.5\n",
    "            FitResults.loc[PixelGroup, \"Azimuth\"] = -180 - FitResults.loc[PixelGroup, \"Azimuth\"]\n",
    "            FitResults.loc[PixelGroup, \"TwoTheta\"] = -1 * abs(FitResults.loc[PixelGroup, \"TwoTheta\"])\n",
    "        if FitResults.loc[PixelGroup, \"Azimuth\"] > 90:        # 163 or 171.5, corresponds to 17 or 8.5\n",
    "            FitResults.loc[PixelGroup, \"Azimuth\"] = 180 - FitResults.loc[PixelGroup, \"Azimuth\"]\n",
    "            FitResults.loc[PixelGroup, \"TwoTheta\"] = -1 * abs(FitResults.loc[PixelGroup, \"TwoTheta\"])\n",
    "        if abs(FitResults.loc[PixelGroup, \"TwoTheta\"]) < 10:  # Degenerate case gives erroneous values - form of gimbal lock?\n",
    "            FitResults.loc[PixelGroup, \"Azimuth\"] = 0\n",
    "            FitResults.loc[PixelGroup, \"TwoTheta\"] = -90\n",
    "            \n",
    "    return FitResults, name\n",
    "    \n",
    "def PlotPeaksAcrossBanks(FitResults, PeakList, Variables=[\"PeakCentre\", \"Sigma\"]):\n",
    "    NumPeaks = len(PeakList)\n",
    "    NumVars = len(Variables)\n",
    "\n",
    "    # Create a gridspec object with the desired number of rows and columns\n",
    "    #gs = gridspec.GridSpec(NumPeaks, NumVars * 2)\n",
    "    fig, axs = plt.subplots(NumPeaks, NumVars)\n",
    "    for Azimuth in [-17, -8.5, 0, 8.5, 17]:\n",
    "        Angles = FitResults[np.isclose(FitResults[\"Azimuth\"], Azimuth)]\n",
    "        for PeakIndex in range(NumPeaks):\n",
    "            for VarIndex in range(NumVars):\n",
    "                #CurPlot=plt.subplot(gs[PeakIndex, VarIndex])\n",
    "                VariableName = f\"{PeakList.index[PeakIndex]}-{Variables[VarIndex]}\"\n",
    "                axs[PeakIndex, VarIndex].plot(Angles[\"TwoTheta\"],Angles[VariableName]) \n",
    "                #CurPlot.plot(Angles[\"TwoTheta\"],\"Cu111-Sigma\") #x=\"Sigma\",\n",
    "        \n",
    "    \n",
    "    \n",
    "# fig, axes = plt.subplots(edgecolor='#ffffff', num='DiffnDataWS-dSp-grp-reb-Fit', subplot_kw={'projection': 'mantid'})\n",
    "# axes.tick_params(axis='x', which='major', **{'gridOn': False, 'tick1On': True, 'tick2On': False, 'label1On': True, 'label2On': False})\n",
    "# axes.tick_params(axis='y', which='major', **{'gridOn': False, 'tick1On': True, 'tick2On': False, 'label1On': True, 'label2On': False})\n",
    "# axes.set_xlabel('axis-1')\n",
    "# axes.set_ylabel('Integrated Intensity')\n",
    "# legend = axes.legend().set_draggable(True).legend\n",
    "\n",
    "# plt.show()\n",
    "# # Scripting Plots in Mantid:\n",
    "# # https://docs.mantidproject.org/tutorials/python_in_mantid/plotting/02_scripting_plots.html\n",
    "#GroupingFile = os.path.join(r'C:/MantidInstall69/scripts/Engineering/calib', 'ENGINX_Texture30_grouping.xml') # ENGINX_5x24.xml ENGINX_Texture30_grouping.xml\n",
    "GroupingFile = os.path.join(r'C:\\MantidInstall\\scripts\\Engineering\\calib', 'ENGINX_Texture30_grouping.xml')\n",
    "\n",
    "df, nm =PeakAcrossBanks(346541, peak_list=[2.0881386024697948, 1.809843123112585, 1.276197610281563, 1.0906672907100898,\n",
    "                                          1.0437930226911487, 0.9044813869705196, 0.8294169997234041, 0.8294169997234041,\n",
    "                                          0.7379630082738616, 0.6953500373475515, 0.6114352638311255], PixelGrouping=GroupingFile, RebinParams=-0.0005)   #193749\n",
    " \n",
    "#df.plot(x=\"TwoTheta\",y=\"Cu111-Sigma\") #x=\"Sigma\",\n",
    "#df.sort_values(by=\"Integrated Intensity\")[\"Integrated Intensity\"].reindex()\n",
    "df\n",
    "#df.sort_values(by=\"Integrated Intensity\", inplace=True)\n",
    "#df[\"Intensity rank\"] = range(240)\n",
    "# #df[\"Integrated Intensity\"].plot(x=\"Intensity rank\")\n",
    "#df.plot(y=\"Integrated Intensity\",x=\"Intensity rank\")\n",
    "#FitResults = df\n",
    "#PlotPeaksAcrossBanks(FitResults, PeakList[:\"F310\"])\n",
    "#FitResults[np.isclose(FitResults[\"Azimuth\"], 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6ffd71-5f35-4f60-ad56-11b4b527c54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mantid.simpleapi import *\n",
    "\n",
    "def MakeGroupingFile(HorizDivs, OutputWkspName='GrpWksp', OutputFilename=None):\n",
    "    CreateGroupingWorkspace(InstrumentName='ENGINX',OutputWorkspace=OutputWkspName)\n",
    "    NumGroups = 10 * HorizDivs # 10 for 2*5 rows of detector modules on ENGIN-X\n",
    "    PixelsInGroup = int(2400 / NumGroups)\n",
    "    GroupList = []\n",
    "    for GroupNo in range(1,NumGroups+1):\n",
    "        GroupList = GroupList + [GroupNo] * PixelsInGroup\n",
    "    print(GroupList)\n",
    "    for i in range(len(GroupList)):\n",
    "        mtd[OutputWkspName].setY(i, [GroupList[i]])\n",
    "    if OutputFilename:\n",
    "        SaveDetectorsGrouping(InputWorkspace=OutputWkspName, OutputFile=OutputFilename)\n",
    "        \n",
    "MakeGroupingFile(24, OutputFilename=os.path.join(GroupingFiles, \"ENGINX_5x24.xml\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1d8736-ef51-4b1e-af2c-2697d18e5ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661cbc88-5724-4bae-a6f4-acae509cd25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "#from scipy.stats import pearsonr\n",
    "import scipy.stats as ss\n",
    "from matplotlib.table import table\n",
    "\n",
    "def AnalyseOrientationRelations(FitResDF,name,corr=\"Pearson\"):\n",
    "    \"\"\"Look for orientation relationships in dataframe of peak fit results\"\"\"\n",
    "    currdir = os.getcwd()\n",
    "    savedir = os.path.join(currdir, \"Saves\")\n",
    "    figName = f'{name}_Orientation_{corr}_Correlation.png'\n",
    "    filedir = os.path.join(savedir, str(name))\n",
    "    figdir = os.path.join(filedir, figName)\n",
    "    Peaks = list({ColNamePart[0] for ColNamePart in df.columns.str.split(\"-\") if len(ColNamePart) == 2}) # PeakList.index.to_numpy() #This retrieves the peaks\n",
    "    NumPeaks = len(Peaks) #How many peaks there are\n",
    "    Correlations = np.zeros((NumPeaks, NumPeaks)) #Matrix of size\n",
    "    for PeakInd1 in range(NumPeaks):\n",
    "        for PeakInd2 in range(NumPeaks):\n",
    "            if corr == \"Pearson\":\n",
    "                #The scipy and numpy pearson coefficients are interchangable, but the scipy one is easier to extract\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.pearsonr(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0]\n",
    "                #print(np.corrcoef(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0][1])\n",
    "                #Correlations[PeakInd1,PeakInd2] = np.corrcoef(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0][1]\n",
    "            elif corr == \"Spearman\":\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.spearmanr(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0]\n",
    "                print(ss.spearmanr(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity']))\n",
    "            elif corr == \"Kendall\":\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.kendalltau(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0]\n",
    "                #print(ss.kendalltau(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity']))\n",
    "            elif corr == \"Regression\":\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.linregress(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity']).rvalue\n",
    "                #print(ss.linregress(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity']))\n",
    "            elif corr == \"Xi\":\n",
    "                #Need to update to current version of scipy for this function to work. Recent coefficient for correlation which works on more complicated functions,\n",
    "                #and may thus be more accurate.\n",
    "                #https://souravchatterjee.su.domains/beam-correlation-trans.pdf\n",
    "                #print(ss.chatterjeexi(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'],nan_policy='propagate'))\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.chatterjeexi(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'],nan_policy='propagate')[0]\n",
    "            elif corr == \"Weighted Kendall\":\n",
    "                #print(ss.weightedtau(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity']))\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.weightedtau(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0]\n",
    "            else:\n",
    "                Correlations[PeakInd1,PeakInd2] = ss.pearsonr(FitResDF[f'{Peaks[PeakInd1]}-Integrated Intensity'],FitResDF[f'{Peaks[PeakInd2]}-Integrated Intensity'])[0]\n",
    "    fig, axes = plt.subplots(1,1)\n",
    "    axes.axis('off')\n",
    "    CorrelationsNormalised = (Correlations + 1) / 2.\n",
    "    CellColours = plt.cm.RdYlGn(CorrelationsNormalised)\n",
    "    CorrTable = plt.table(cellText=Correlations.round(decimals=3), cellColours=CellColours, loc='center', cellLoc='center',colLabels=Peaks, rowLabels=Peaks) # ,edges='open'\n",
    "    CorrTable.set_fontsize(20)\n",
    "    sortedCorr = sortCorrelations(Correlations,Peaks,NumPeaks)\n",
    "    saveOrientation(name,corr,Correlations,NumPeaks,sortedCorr,Peaks)\n",
    "    plt.savefig(figdir)\n",
    "    #return Correlations\n",
    "    #print(dir(CorrTable))\n",
    "    #return Peaks\n",
    "\n",
    "def sortCorrelations(Correlations,peaks,peaklen):\n",
    "    #print(peaks)\n",
    "    sortedDic = {}\n",
    "    i = 0\n",
    "    for peak in peaks:\n",
    "        peakCorr = []\n",
    "        #pOName = f'peak_{i}_ordered'\n",
    "        orderedPeaks = []\n",
    "        for j in range(peaklen):\n",
    "            peakCorr.append(Correlations[i,j])\n",
    "        peakCorrSorted = np.sort(peakCorr)\n",
    "        peakCorrSorted = np.flip(peakCorrSorted)\n",
    "        for sortedPeak in peakCorrSorted:\n",
    "            orderedPeaks.append(peaks[int(np.argwhere(peakCorr == sortedPeak))])\n",
    "        sortedDic[f'{peak}_ordered'] = orderedPeaks\n",
    "        i = i + 1\n",
    "    return sortedDic\n",
    "            \n",
    "#For some reason these runs aren't providing peaks\n",
    "#Why these runs specifically - ask Joe\n",
    "#These aren't cubic and so can't be fitted right now\n",
    "df,nm = PeakAcrossBanks(216884, PixelGrouping=GroupingFile, RebinParams=-0.0001)\n",
    "AnalyseOrientationRelations(df,nm,corr=\"Pearson\")\n",
    "df,nm = PeakAcrossBanks(216885, PixelGrouping=GroupingFile, RebinParams=-0.0001)\n",
    "AnalyseOrientationRelations(df,nm,corr=\"Pearson\")\n",
    "df,nm = PeakAcrossBanks(216886, PixelGrouping=GroupingFile, RebinParams=-0.0001)\n",
    "AnalyseOrientationRelations(df,nm,corr=\"Pearson\")\n",
    "\n",
    "#df,nm = PeakAcrossBanks(351364, PixelGrouping=GroupingFile, RebinParams=-0.0001)\n",
    "#print(df)\n",
    "#AnalyseOrientationRelations(df,nm,corr=\"Pearson\")\n",
    "#df,nm = PeakAcrossBanks(351464, PixelGrouping=GroupingFile, RebinParams=-0.0001)\n",
    "#AnalyseOrientationRelations(df,nm,corr=\"Pearson\")\n",
    "#df,nm = PeakAcrossBanks(351538, PixelGrouping=GroupingFile, RebinParams=-0.0001)\n",
    "#AnalyseOrientationRelations(df,nm,corr=\"Pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea28962-ff7b-472a-84fa-2b59e69edde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remaking this cell with lmfit instead of curve_fit\n",
    "import math as m\n",
    "\n",
    "def AnalyseGrainSize(FitResDF, name, title=\"\"):\n",
    "    Peaks = list({ColNamePart[0] for ColNamePart in df.columns.str.split(\"-\") if len(ColNamePart) == 2}) # PeakList.index.to_numpy()\n",
    "    NumPeaks = len(Peaks)\n",
    "    fig, axes = plt.subplots(1,1)\n",
    "    Sizes = {}\n",
    "    grain_grads = {}\n",
    "    for PeakInd in range(NumPeaks):\n",
    "        IntensitySeries = f'{Peaks[PeakInd]}-Integrated Intensity'\n",
    "        FitResDF[IntensitySeries] = FitResDF[IntensitySeries].mask(~FitResDF[IntensitySeries].between(1e-4,1e+4))\n",
    "    #FitResDF.dropna(inplace=True)\n",
    "    currdir = os.getcwd()\n",
    "    savedir = os.path.join(currdir, \"Saves\")\n",
    "    figName = f'{title}_GrainSize_graph.png'\n",
    "    filedir = os.path.join(savedir, str(name))\n",
    "    figdir = os.path.join(filedir, figName)\n",
    "    for PeakInd in range(NumPeaks):\n",
    "        Intensities = FitResDF[f'{Peaks[PeakInd]}-Integrated Intensity'].dropna().to_numpy()\n",
    "        #print(Intensities)\n",
    "        Intensities.sort()\n",
    "        Intensities = Intensities[-2:2:-1]\n",
    "        IntensityIndexes = np.array(range(len(Intensities)))\n",
    "        plt.yscale('log')\n",
    "        plt.ylim(1e-3, 3)\n",
    "        plt.title(f'{title}')\n",
    "        #plt.plot(Intensities)\n",
    "        if len(Intensities) >= 2:\n",
    "            #print(IntensityIndexes,Intensities)\n",
    "            #How are these decided?\n",
    "            a_const = 10\n",
    "            grain_param = 1\n",
    "            pfitNE = lm.create_params(a_psi = a_const, psi = grain_param)\n",
    "            miniNE = lm.Minimizer(residualNE, pfitNE, fcn_args=(IntensityIndexes,Intensities),nan_policy='propagate')\n",
    "            outNE = miniNE.leastsq()\n",
    "            pfitCP = lm.create_params(a_alpha = a_const, alpha = grain_param)\n",
    "            miniCP = lm.Minimizer(residualCP, pfitCP, fcn_args=(IntensityIndexes,Intensities),nan_policy='propagate')\n",
    "            outCP = miniCP.leastsq()\n",
    "            pfitWL = lm.create_params(a_w = a_const, w = grain_param)\n",
    "            miniWL = lm.Minimizer(residualWL, pfitWL, fcn_args=(IntensityIndexes,Intensities),nan_policy='propagate')\n",
    "            outWL = miniWL.leastsq()\n",
    "            nameComp = [\"NE\",\"CP\",\"WL\"]\n",
    "            fitComp = [abs(np.sum(outNE.residual)), abs(np.sum(outCP.residual)), abs(np.sum(outWL.residual))]\n",
    "            bestOC = np.amin(fitComp)\n",
    "            #print(bestOC, np.argwhere(fitComp == bestOC))\n",
    "            fit = nameComp[np.amin(np.argwhere(fitComp == bestOC))]\n",
    "            #print(fitComp, fit)\n",
    "            if fit == \"NE\":\n",
    "                best_fit = Intensities + outNE.residual\n",
    "                psiFin = outNE.params['psi'].value\n",
    "                plt.plot(Intensities, label=f'{PeakInd}--{round(psiFin,2)}({fit})')\n",
    "                plt.plot(best_fit, color='black', linestyle='dashed')\n",
    "                #Need to figure out coords more before using annotate\n",
    "                #plt.annotate(text=f'{round(psiFin,2)}',xy=(IntensityIndexes[0],best_fit[0]))\n",
    "                plt.legend()\n",
    "                Sizes[Peaks[PeakInd]] = outNE.params['psi'].value # Second parameter is gradient-like parameter that relates to the sizes. Might be changed by lmfit\n",
    "            elif fit == \"CP\":\n",
    "                aFin = outCP.params['alpha'].value\n",
    "                best_fit = Intensities + outCP.residual\n",
    "                plt.plot(Intensities, label=f'{PeakInd}--{round(aFin,2)}({fit})')\n",
    "                plt.plot(best_fit, color='black', linestyle='dashed')\n",
    "                #plt.annotate(text=f'{round(aFin,2)}',xy=(IntensityIndexes[0],best_fit[0]))\n",
    "                plt.legend()\n",
    "                Sizes[Peaks[PeakInd]] = outCP.params['alpha'].value\n",
    "            elif fit == \"WL\":\n",
    "                wFin = outWL.params['w'].value\n",
    "                best_fit = Intensities + outWL.residual\n",
    "                plt.plot(Intensities, label=f'{PeakInd}--{round(wFin,2)}({fit})')\n",
    "                plt.plot(best_fit, color='black', linestyle='dashed')\n",
    "                #plt.annotate(text=f'{round(wFin,2)}',xy=(IntensityIndexes[0],best_fit[0]))\n",
    "                plt.legend()\n",
    "                Sizes[Peaks[PeakInd]] = outWL.params['w'].value\n",
    "        else:\n",
    "            Sizes[Peaks[PeakInd]] = None\n",
    "    plt.savefig(figdir)\n",
    "    return Sizes\n",
    "    \n",
    "def NegativeExp(x, a_psi, psi):\n",
    "    return a_psi*np.exp(-(x/psi))\n",
    "def residualNE(pars,x,data):\n",
    "    model = NegativeExp(x, a_psi=pars['a_psi'],psi=pars['psi']) \n",
    "    return model - data\n",
    "    \n",
    "def ConstPower(x, a_alpha, alpha):\n",
    "    #return a_alpha*(x**(-float(alpha)))\n",
    "    #Temporary fix. This function specifically causes a lot of issue with nans\n",
    "    return a_alpha*(x**(int(alpha)))\n",
    "def residualCP(pars,x,data):\n",
    "    model = ConstPower(x, a_alpha=pars['a_alpha'],alpha=pars['alpha']) \n",
    "    return model - data\n",
    "\n",
    "def WLorentz(x, a_w, w):\n",
    "    return a_w*(w**2/(w**2+x**2))\n",
    "def residualWL(pars,x,data):\n",
    "    model = WLorentz(x, a_w=pars['a_w'],w=pars['w']) \n",
    "    return model - data\n",
    "\n",
    "\n",
    "#df.plot(x=\"TwoTheta\",y=\"PeakCentre\") #x=\"Sigma\",\n",
    "#df.sort_values(by=\"Integrated Intensity\")[\"Integrated Intensity\"].reindex()\n",
    "#df\n",
    "# df.sort_values(by=\"Integrated Intensity\", inplace=True)\n",
    "# df[\"Intensity rank\"] = range(240)\n",
    "# #df[\"Integrated Intensity\"].plot(x=\"Intensity rank\")\n",
    "\n",
    "#GroupingFile = os.path.join(r'C:/MantidInstall69/scripts/Engineering/calib', 'ENGINX_Texture30_grouping.xml') # ENGINX_5x24.xml ENGINX_Texture30_grouping.xml\n",
    "GroupingFile = os.path.join(r'C:\\MantidInstall\\scripts\\Engineering\\calib', 'ENGINX_Texture30_grouping.xml')\n",
    "\n",
    "sizes = []\n",
    "#UniquePointRange = list(range(324884,324934))\n",
    "#for duplicate in [324897,324899,324905,324907,324913,324915]:\n",
    "#    UniquePointRange.remove(duplicate)\n",
    "#print(UniquePointRange)\n",
    "UniquePointRange = list(range(351464,351470))\n",
    "for RunNo in UniquePointRange: # range(324884,324975):   # from 324884\n",
    "    df,nm = PeakAcrossBanks(RunNo, PixelGrouping=GroupingFile, RebinParams=-0.0005)\n",
    "    grainsize = AnalyseGrainSize(df, nm, title=RunNo)\n",
    "    grainsize[\"X\"]=mtd[\"DiffnDataWS\"].getRun()[\"x_position\"].timeAverageValue()\n",
    "    grainsize[\"Y\"]=mtd[\"DiffnDataWS\"].getRun()[\"y_position\"].timeAverageValue()\n",
    "    grainsize[\"Z\"]=mtd[\"DiffnDataWS\"].getRun()[\"z_position\"].timeAverageValue()\n",
    "    grainsizedf = pd.DataFrame(grainsize, index=[RunNo])\n",
    "    sizes.append(grainsizedf)\n",
    "\n",
    "SizeDF = pd.concat(sizes)\n",
    "#fig, axes = plt.subplots(1,1)\n",
    "#plt.plot(SizeDF)\n",
    "SizeDF\n",
    "# df.plot(y=\"Integrated Intensity\",x=\"Intensity rank\")\n",
    "#df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbea54a-f118-42cc-ae18-db309244dd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(SizeDF.keys())\n",
    "SizeDF = SizeDF.fillna(0)\n",
    "print(SizeDF)\n",
    "#AnalyseGrainSize(df,nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d3cde5-28d9-4c0c-bf1c-0ee8193966e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Contour plotter using XYZ data in Excel\n",
    "\n",
    "Joe Kelleher 2010\n",
    "\n",
    "'''\n",
    "%matplotlib ipympl\n",
    "import win32com.client\n",
    "from numpy import *\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as path\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "\n",
    "XSteps = 200\n",
    "YSteps = 200\n",
    "#XSteps = 10\n",
    "#YSteps = 10\n",
    "\n",
    "#MinValue = None #-250 # Can be set to None for auto scaling\n",
    "#MaxValue = None #250\n",
    "#ContourLevels = arange(MinValue, MaxValue+1, 50)\n",
    "\n",
    "LengthScalesOnImage = False\n",
    "\n",
    "XAxisFontSize = 16\n",
    "YAxisFontSize = 16\n",
    "ColourBarFontSize = 16\n",
    "# Colour bar position and size changed by numbers below\n",
    "ColourBar_BottomLeft_X = 0.95\n",
    "ColourBar_BottomLeft_Y = 0.2\n",
    "ColourBar_Width = 0.05\n",
    "ColourBar_Height = 0.6\n",
    "\n",
    "'''\n",
    "def PlotXYZfromXL():\n",
    "    XL = GetExcelWorkbook(\"SSPL data\")\n",
    "    XYZData = ReadExcelList(XL, \"SSPL data\", \"A4:C4\", ExtendDown=True)\n",
    "    ShapeData = ReadExcelList(XL, \"Shape\", \"A2:B2\", ExtendDown=True)\n",
    "    print(XYZData)\n",
    "    #XValues, YValues, ZTable, FValues = BuildTableFromList(XYZData[0], XYZData[1], XYZData[2])\n",
    "    #XYF = XYZData[0:3]\n",
    "    #XValues, YValues, FValues = XYF\n",
    "    XValues, YValues, ZValues = XYZData[:,0], XYZData[:,1], XYZData[:,2]\n",
    "    XYF = FindF(XValues, YValues, ZValues)\n",
    "    FValues = XYF[2]\n",
    "    XGridValues = arange(min(ShapeData[:,0]), max(ShapeData[:,0]), (max(ShapeData[:,0]) - min(ShapeData[:,0])) / XSteps)\n",
    "    YGridValues = arange(min(ShapeData[:,1]), max(ShapeData[:,1]), (max(ShapeData[:,1]) - min(ShapeData[:,1])) / YSteps)\n",
    "    print(XGridValues, YGridValues)\n",
    "    #XGrid, YGrid = meshgrid(XGridValues, YGridValues)\n",
    "    #print XGrid.ravel()\n",
    "    #print YGrid.ravel()\n",
    "    #ZGrid = EvaluateZWithVectors(XGrid.ravel(), YGrid.ravel(), XYF)\n",
    "    #print type(ZGrid)\n",
    "    # Evaluate row-at-a-time due to memory constraints\n",
    "    ZGrid = array([[]]* len(XGridValues))\n",
    "    for YGridValue in YGridValues[::1]:\n",
    "        ZGridForY = EvaluateZ(XGridValues, array([YGridValue] * len(XGridValues)), XYF)\n",
    "        ZGrid = concatenate((ZGrid, ZGridForY.reshape((-1, 1))), axis=1)\n",
    "    MakeContourPlot(XGridValues, YGridValues, ZGrid.transpose(), Outline=ShapeData, MarkerPoints=XYF[0:2])\n",
    "    #MakeContourPlot(XGridValues, YGridValues, ZGrid, Outline=ShapeData, MarkerPoints=XYF[0:2])\n",
    "    #patch = matplotlib.patches.Circle((10,10), radius=100)\n",
    "    #im.set_clip_path(patch)\n",
    "'''\n",
    "\n",
    "def MakeContourPlot(XGridValues, YGridValues, ZGrid, ContourLevels, Outline=None, MarkerPoints=None):\n",
    "    fig = plt.figure()\n",
    "    if LengthScalesOnImage:\n",
    "       ax = fig.add_subplot(111, frameon=False) # , xticks=[], yticks=[]\n",
    "    else:\n",
    "        ax = fig.add_subplot(111, frameon=False, xticks=[], yticks=[])\n",
    "    ContourLines = ax.contour(XGridValues, YGridValues, ZGrid, colors = 'k', levels=ContourLevels)\n",
    "    #for c in ContourLines.collections:\n",
    "    #    c.set_linestyle('solid') # Don't use dashed line for <0 contours\n",
    "    ImageExtent = (XGridValues[0], XGridValues[-1], YGridValues[0], YGridValues[-1])\n",
    "    BackgroundImage = ax.imshow(ZGrid, extent=ImageExtent, origin='lower', vmin=min(ContourLevels), vmax=max(ContourLevels))\n",
    "    if Outline != None:\n",
    "        #OutlinePath = path.Path(vertices=tuple(Outline))\n",
    "        OutlinePath = patches.Polygon(Outline, closed=True, facecolor='none')\n",
    "        ax.add_patch(OutlinePath)\n",
    "        BackgroundImage.set_clip_path(OutlinePath)\n",
    "        ax.set_clip_path(OutlinePath)\n",
    "        #ax.set_clip_on(True)\n",
    "        #for c in ContourLines.collections:\n",
    "        #    c.set_clip_path(OutlinePath)\n",
    "        #    c.set_clip_on(True)\n",
    "    #ax.set_xticks(np.linspace(0,len(XValues),num=10,dtype=float), labels=np.around(np.linspace(XValues[0],XValues[-1],num=10,dtype=float),decimals=2))\n",
    "    #ax.set_yticks(np.linspace(0,len(YValues),num=10,dtype=float), labels=np.around(np.linspace(YValues[0],YValues[-1],num=10,dtype=float),decimals=2))\n",
    "    ColourBarAxes = fig.add_axes([ColourBar_BottomLeft_X, ColourBar_BottomLeft_Y, ColourBar_Width, ColourBar_Height])\n",
    "    ColorBar = fig.colorbar(BackgroundImage, cax=ColourBarAxes)\n",
    "    if MarkerPoints != None:\n",
    "        MarkerPoints = ax.scatter(reshape(MarkerPoints[0],-1),reshape(MarkerPoints[1],-1),marker='x',c='k',s=0.05,zorder=10)\n",
    "    for label in ax.xaxis.get_ticklabels():\n",
    "        # label is a Text instance\n",
    "        label.set_fontsize(XAxisFontSize)\n",
    "    for label in ax.yaxis.get_ticklabels():\n",
    "        # label is a Text instance\n",
    "        label.set_fontsize(YAxisFontSize)\n",
    "    for label in ColorBar.ax.yaxis.get_ticklabels():\n",
    "        # label is a Text instance\n",
    "        label.set_fontsize(ColourBarFontSize)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def FFromZMatrix(KnownX, KnownY):\n",
    "    KnownXRow = reshape(KnownX, -1)\n",
    "    KnownYRow = reshape(KnownY, -1)\n",
    "    DiffX = KnownXRow[:,newaxis] - KnownXRow[:]\n",
    "    DiffY = KnownYRow[:,newaxis] - KnownYRow[:]\n",
    "    r2 = (DiffX ** 2) + (DiffY ** 2)\n",
    "    RadialBasis = r2 * log(r2 + 0.0000001)\n",
    "    PlaneFit = concatenate(([ones(KnownXRow.shape[0])], [KnownXRow], [KnownYRow]), 0)   # Three rows at bottom\n",
    "    FFromZ = concatenate((concatenate((RadialBasis, PlaneFit), 0), \n",
    "                       concatenate((transpose(PlaneFit), zeros([3, 3])), 0)), 1)\n",
    "    return FFromZ\n",
    "\n",
    "\n",
    "def FindF(KnownX, KnownY, KnownZ):\n",
    "    KnownXRow = reshape(KnownX, -1)\n",
    "    KnownYRow = reshape(KnownY, -1)\n",
    "    KnownZRow = concatenate((reshape(KnownZ, -1), zeros(3)), 0)\n",
    "    SolutionMatrix = FFromZMatrix(KnownXRow, KnownYRow)\n",
    "    F = reshape(linalg.lstsq(SolutionMatrix, KnownZRow)[0], -1)\n",
    "    return [KnownXRow, KnownYRow, F]\n",
    "\n",
    "def EvaluateZ(SoughtX, SoughtY, XYF):\n",
    "    SoughtXCol = reshape(SoughtX, (-1, 1))\n",
    "    SoughtYCol = reshape(SoughtY, (-1, 1))\n",
    "    ZFromF = concatenate((\n",
    "                ((SoughtXCol - XYF[0]) ** 2 + (SoughtYCol - XYF[1]) ** 2) *\n",
    "                   log((SoughtXCol - XYF[0]) ** 2 + (SoughtYCol - XYF[1]) ** 2 + 0.0000001),\n",
    "                ones((SoughtXCol.shape[0], 1)),\n",
    "                SoughtXCol,\n",
    "                SoughtYCol), 1)\n",
    "    ZValues = dot(ZFromF, XYF[2])\n",
    "    return ZValues\n",
    "\n",
    "def EvaluateZWithVectors(SoughtX, SoughtY, XYF):\n",
    "    SoughtXCol = reshape(SoughtX, (-1, 1))\n",
    "    SoughtYCol = reshape(SoughtY, (-1, 1))\n",
    "    \n",
    "    #SoughtXYArray = array([SoughtXCol, SoughtYCol])\n",
    "    #KnownXYArray = array([[XYF[0]], [XYF[1]]])\n",
    "    XDifferencesSq = (SoughtXCol - XYF[0]) ** 2\n",
    "    YDifferencesSq = (SoughtYCol - XYF[1]) ** 2\n",
    "    #Test = Test ** 2\n",
    "    RSquared = XDifferencesSq + YDifferencesSq #sum((SoughtXYArray - KnownXYArray)**2,1)\n",
    "    RSqLnRSq = RSquared * log(RSquared + 0.0000001) # Epsilon\n",
    "    ZValues = XYF[2][-3] + XYF[2][-2] * SoughtXCol + XYF[2][-1] * SoughtYCol \\\n",
    "        + sum(XYF[2][:-3] * RSqLnRSq)\n",
    "    #StressTensor = StressData[-3,3:8:2] + XPosition * StressData[-2,3:8:2] \\\n",
    "    #                                    + YPosition * StressData[-1,3:8:2] \\\n",
    "    #               + sum(StressData[0:-3,3:8:2] * transpose(array([RSqLnRSq,RSqLnRSq,RSqLnRSq])))\n",
    "    #    ZFromF = concatenate((\n",
    "    #                ((SoughtXCol - XYF[0]) ** 2 + (SoughtYCol - XYF[1]) ** 2) *\n",
    "    #                   log((SoughtXCol - XYF[0]) ** 2 + (SoughtYCol - XYF[1]) ** 2 + 0.0000001),\n",
    "    #                ones((SoughtXCol.shape[0], 1)),\n",
    "    #                SoughtXCol,\n",
    "    #                SoughtYCol), 1)\n",
    "    #    ZValues = matrixmultiply(ZFromF, XYF[2])\n",
    "    return ZValues\n",
    "\n",
    "def StressAtPoint(self, XPosition, YPosition, StressData):\n",
    "    #Epsilon = 0.0000001\n",
    "    # First work out StressX, StressY, StressShear\n",
    "    # StressTensor is [ StressX, StressY, StressShear ]\n",
    "    #StressX = StressData[-3,3] + XPosition * StressData[-2,3] \\\n",
    "    #                                        + YPosition * StressData[-1,3]\n",
    "    #StressY = StressData[-3,5] + XPosition * StressData[-2,5] \\\n",
    "    #                                        + YPosition * StressData[-1,5]\n",
    "    #StressShear = StressData[-3,7] + XPosition * StressData[-2,7] \\\n",
    "    #                                            + YPosition * StressData[-1,7]\n",
    "    #RSquared = (XPosition - StressData[0:-3,0]) ** 2 + (YPosition - StressData[0:-3,1]) ** 2\n",
    "    #RSqLnRSq = RSquared * log(RSquared + 0.0000001) # Epsilon\n",
    "    RSquared = sum(((XPosition, YPosition) - StressData[0:-3,0:2])**2,1)\n",
    "    RSqLnRSq = RSquared * log(RSquared + 0.0000001) # Epsilon\n",
    "    StressTensor = StressData[-3,3:8:2] + XPosition * StressData[-2,3:8:2] \\\n",
    "                                        + YPosition * StressData[-1,3:8:2] \\\n",
    "                   + sum(StressData[0:-3,3:8:2] * transpose(array([RSqLnRSq,RSqLnRSq,RSqLnRSq])))\n",
    "    # StressTensor = StressTensor + ...\n",
    "    #StressX = StressX + Numeric.sum(StressData[0:-3,3] * RSqLnRSq)\n",
    "    #StressY = StressY + Numeric.sum(StressData[0:-3,5] * RSqLnRSq)\n",
    "    #StressShear = StressShear + Numeric.sum(StressData[0:-3,7] * RSqLnRSq)\n",
    "    return StressTensor\n",
    "\n",
    "\n",
    "def BuildTableFromList(XValues, YValues, ZValues=None, FValues=None):\n",
    "    if FValues == None:\n",
    "        XYF = FindF(XValues, YValues, ZValues)\n",
    "        FValues = XYF[2]\n",
    "    XRange = max(XValues) - min(XValues)\n",
    "    YRange = max(YValues) - min(YValues)\n",
    "    XGridValues = arange(min(XValues), max(XValues), XRange / XSteps)\n",
    "    YGridValues = arange(min(YValues), max(YValues), YRange / YSteps)\n",
    "    XGrid, YGrid = meshgrid(XGridValues, YGridValues)\n",
    "    print(XGrid.ravel())\n",
    "    print(YGrid.ravel())\n",
    "    ZGrid = EvaluateZ(XGrid.ravel(), YGrid.ravel(), XYF) #WithVectors\n",
    "    ZGrid = ZGrid.reshape((len(XGridValues), len(YGridValues)))\n",
    "    #print(ZGrid.ravel())\n",
    "    return XGridValues, YGridValues, ZGrid, XYF\n",
    "\n",
    "def IsInShape(self, XPosition, YPosition):\n",
    "    #pywin.debugger.brk()\n",
    "    NumberOfPoints = len(self.EdgePoints)\n",
    "    EdgesToLeft = 0\n",
    "    CurrentPoint = 0\n",
    "    Y2 = self.EdgePoints[-1][1]\n",
    "    while CurrentPoint < NumberOfPoints:\n",
    "        # Get Y positions, Y1 and Y2 of two consecutive points in the list\n",
    "        Y1 = Y2\n",
    "        Y2 = self.EdgePoints[CurrentPoint][1]\n",
    "        # If requested Y value lies between them, work out X position on this edge with\n",
    "        # the same Y value as the requested Y value\n",
    "        if (Y2 >= YPosition) == (YPosition > Y1):\n",
    "            X1 = self.EdgePoints[CurrentPoint - 1][0]  # Final value when CurrentPoint = 0\n",
    "            X2 = self.EdgePoints[CurrentPoint][0]\n",
    "            EdgeXPosition = X1 + ((X2 - X1) * ((YPosition - Y1) / (Y2 - Y1)))\n",
    "            if EdgeXPosition <= XPosition:\n",
    "                EdgesToLeft = EdgesToLeft + 1\n",
    "        CurrentPoint = CurrentPoint + 1\n",
    "    # Final answer depends on whether an even or odd number of edges were crossed\n",
    "    if EdgesToLeft % 2 == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "'''\n",
    "def ReadExcelList(XLWorkbook, XLWorksheetName, XLCellRange, ExtendDown=True):\n",
    "    RangeTop = XLWorkbook.Sheets(XLWorksheetName).Range(XLCellRange)\n",
    "    ColumnCount = RangeTop.Columns.Count\n",
    "    print(ColumnCount, \" columns\")\n",
    "    ExtraRows = 0\n",
    "    if ExtendDown:\n",
    "        while RangeTop.Cells(101 + ExtraRows,1).Value != None:\n",
    "            ExtraRows = ExtraRows + 100\n",
    "        while RangeTop.Cells(11 + ExtraRows,1).Value != None:\n",
    "            ExtraRows = ExtraRows + 10\n",
    "        while RangeTop.Cells(1 + ExtraRows,1).Value != None:\n",
    "            ExtraRows = ExtraRows + 1\n",
    "    RangeCells = XLWorkbook.Sheets(XLWorksheetName).Range(RangeTop.Cells(1,1),\n",
    "                                                              RangeTop.Cells(ExtraRows, ColumnCount)).Value\n",
    "    return array(RangeCells)\n",
    "\n",
    "def GetExcelWorkbook(WorksheetName):\n",
    "    \"Get a COM reference to a workbook with a given worksheet name\"\n",
    "    XL = win32com.client.Dispatch(\"Excel.Application\")\n",
    "    XL.Visible = 1\n",
    "    XLBooksCount = XL.Workbooks.Count\n",
    "    FoundBook = False\n",
    "    for XLBookNo in range(1, XLBooksCount + 1):\n",
    "        try:\n",
    "            XLSheet = XL.Workbooks(XLBookNo).Sheets(WorksheetName)\n",
    "            # If that didn't generate exception, we've found a workbook with the sheet in it\n",
    "            FoundBook = True\n",
    "            print(\"Found already open file...\")\n",
    "        except:\n",
    "            pass\n",
    "        if FoundBook == True:\n",
    "            break\n",
    "    XLBook = XL.Workbooks(XLBookNo)\n",
    "    print(XLBook.Name)\n",
    "    XL.ScreenUpdating = True\n",
    "    return XLBook\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    PlotXYZfromXL()\n",
    "'''\n",
    "54"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
